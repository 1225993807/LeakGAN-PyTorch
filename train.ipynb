{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import json \n",
    "import glob\n",
    "import os #for checkpoint management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_iter import real_data_loader, dis_data_loader\n",
    "from utils import recurrent_func, loss_func, get_sample, get_rewards\n",
    "from Discriminator import Discriminator\n",
    "from Generator import Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_params(filePath):\n",
    "    with open(filePath, 'r') as f:\n",
    "        params = json.load(f)\n",
    "    f.close()\n",
    "    return params\n",
    "\n",
    "def get_arguments():\n",
    "    train_params = get_params(\"./params/train_params.json\")\n",
    "    leak_gan_params = get_params(\"./params/leak_gan_params.json\")\n",
    "    target_params = get_params(\"./params/target_params.json\")\n",
    "    dis_data_params = get_params(\"./params/dis_data_params.json\")\n",
    "    real_data_params = get_params(\"./params/real_data_params.json\")\n",
    "    return {\n",
    "        \"train_params\": train_params,\n",
    "        \"leak_gan_params\": leak_gan_params,\n",
    "        \"target_params\": target_params,\n",
    "        \"dis_data_params\": dis_data_params,\n",
    "        \"real_data_params\" : real_data_params\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#List of models\n",
    "def prepare_model_dict(use_cuda=False):\n",
    "    f = open(\"./params/leak_gan_params.json\")\n",
    "    params = json.load(f)\n",
    "    f.close()\n",
    "    discriminator_params = params[\"discriminator_params\"]\n",
    "    generator_params = params[\"generator_params\"]\n",
    "    worker_params = generator_params[\"worker_params\"]\n",
    "    manager_params = generator_params[\"manager_params\"]\n",
    "    discriminator_params[\"goal_out_size\"] = sum(\n",
    "        discriminator_params[\"num_filters\"]\n",
    "    )\n",
    "    worker_params[\"goal_out_size\"] = discriminator_params[\"goal_out_size\"]\n",
    "    manager_params[\"goal_out_size\"] = discriminator_params[\"goal_out_size\"]\n",
    "    discriminator = Discriminator(**discriminator_params)\n",
    "    generator = Generator(worker_params, manager_params,\n",
    "                          generator_params[\"step_size\"])\n",
    "    if use_cuda:\n",
    "        generator = generator.cuda()\n",
    "        discriminator = discriminator.cuda()\n",
    "    model_dict = {\"generator\": generator, \"discriminator\": discriminator}\n",
    "    return model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#List of optimizers\n",
    "def prepare_optimizer_dict(model_dict, lr_dict): #lr_dict = learning rate \n",
    "    generator = model_dict[\"generator\"]\n",
    "    discriminator = model_dict[\"discriminator\"]\n",
    "    worker = generator.worker\n",
    "    manager = generator.manager\n",
    "\n",
    "    m_lr = lr_dict[\"manager\"]\n",
    "    w_lr = lr_dict[\"worker\"]\n",
    "    d_lr = lr_dict[\"discriminator\"]\n",
    "\n",
    "    w_optimizer = optim.Adam(worker.parameters(), lr=w_lr)\n",
    "    m_optimizer = optim.Adam(manager.parameters(), lr=m_lr)\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=d_lr)\n",
    "\n",
    "    return {\"worker\": w_optimizer, \"manager\": m_optimizer,\n",
    "            \"discriminator\": d_optimizer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#List of Learning rate Schedulers\n",
    "def prepare_scheduler_dict(optmizer_dict, step_size=200, gamma=0.99):\n",
    "    w_optimizer = optmizer_dict[\"worker\"]\n",
    "    m_optimizer = optmizer_dict[\"manager\"]\n",
    "    d_optimizer = optmizer_dict[\"discriminator\"]\n",
    "\n",
    "    w_scheduler = optim.lr_scheduler.StepLR(w_optimizer, step_size=step_size,\n",
    "                                            gamma=gamma)\n",
    "    m_scheduler = optim.lr_scheduler.StepLR(m_optimizer, step_size=step_size,\n",
    "                                            gamma=gamma)\n",
    "    d_scheduler = optim.lr_scheduler.StepLR(d_optimizer, step_size=step_size,\n",
    "                                            gamma=gamma)\n",
    "    return {\"worker\": w_scheduler, \"manager\": m_scheduler,\n",
    "            \"discriminator\": d_scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pretraining the Generator\n",
    "def pretrain_generator(model_dict, optimizer_dict, scheduler_dict, dataloader, vocab_size, max_norm=5.0, use_cuda=False):\n",
    "    #get the models of generator\n",
    "    generator = model_dict[\"generator\"]\n",
    "    worker = generator.worker\n",
    "    manager = generator.worker\n",
    "    #get the optimizers\n",
    "    m_optimizer = optimizer_dict[\"manager\"]\n",
    "    w_optimizer = optimizer_dict[\"worker\"]\n",
    "    \n",
    "    m_optimizer.zero_grad()\n",
    "    w_optimizer.zero_grad()\n",
    "\n",
    "    m_lr_scheduler = scheduler_dict[\"manager\"]\n",
    "    w_lr_scheduler = scheduler_dict[\"worker\"]\n",
    "    \"\"\"\n",
    "     Perform pretrain step for real data\n",
    "    \"\"\"\n",
    "    for i, sample in enumerate(dataloader):\n",
    "        m_lr_scheduler.step()\n",
    "        w_lr_scheduler.step()\n",
    "\n",
    "        sample = Variable(sample)\n",
    "        if use_cuda:\n",
    "            sample = sample.cuda(asyn=True)\n",
    "        \n",
    "        # Calculate pretrain loss\n",
    "        pre_rets = recurrent_func(\"pre\")(model_dict, sample, use_cuda)\n",
    "        real_goal = pre_rets[\"real_goal\"]\n",
    "        prediction = pre_rets[\"prediction\"]\n",
    "        delta_feature = pre_rets[\"delta_feature\"]\n",
    "\n",
    "        m_loss = loss_func(\"pre_manager\")(real_goal, delta_feature)\n",
    "        torch.autograd.grad(m_loss, manager.parameters())\n",
    "        clip_grad_norm(manager.parameters(), max_norm=max_norm)\n",
    "        m_optimizer.step()\n",
    "        m_optimizer.zero_grad()\n",
    "\n",
    "        w_loss = loss_func(\"pre_worker\")(sample, prediction, vocab_size, use_cuda)\n",
    "        torch.autograd.grad(w_loss, worker.parameters())\n",
    "        clip_grad_norm(worker.parameters(), max_norm=max_norm)\n",
    "        w_optimizer.step()\n",
    "        w_optimizer.zero_grad()\n",
    "        print(\"Pre-Manager Loss: {.5f}, Pre-Worker Loss: {:.5f}\\n\".format(m_loss, w_loss))\n",
    "    \"\"\"\n",
    "    Update model_dict, optimizer_dict, and scheduler_dict\n",
    "    \"\"\"\n",
    "\n",
    "    generator.woroker = worker\n",
    "    generator.manager = manager\n",
    "    model_dict[\"generator\"] = generator\n",
    "\n",
    "    optimizer_dict[\"manager\"] = m_optimizer\n",
    "    optimizer_dict[\"worker\"] = w_optimizer\n",
    "\n",
    "    scheduler_dict[\"manager\"] = m_lr_scheduler\n",
    "    scheduler_dict[\"worker\"] = w_lr_scheduler\n",
    "\n",
    "    return model_dict, optimizer_dict, scheduler_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_samples(model_dict, negative_file, batch_size,\n",
    "                     use_cuda=False, temperature=1.0):\n",
    "    neg_data = []\n",
    "    for _ in range(batch_size):\n",
    "        sample = get_sample(model_dict, use_cuda, temperature)\n",
    "        sample = sample.cpu()\n",
    "        neg_data.append(sample.data.numpy())\n",
    "    neg_data = np.concatenate(neg_data, axis=0)\n",
    "    np.save(negative_file, neg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretrain_discriminator(model_dict, optimizer_dict, scheduler_dict,\n",
    "                           dis_dataloader_params, vocab_size, positive_file,\n",
    "                           negative_file, batch_size, epochs, use_cuda=False, temperature=1.0):\n",
    "    discriminator = model_dict[\"discriminator\"]\n",
    "\n",
    "    d_optimizer = optimizer_dict[\"discriminator\"]\n",
    "    d_lr_scheduler = scheduler_dict[\"discriminator\"]\n",
    "\n",
    "    generate_samples(model_dict, negative_file, batch_size, use_cuda, temperature)\n",
    "    dis_dataloader_params[\"positive_filepath\"] = positive_file\n",
    "    dis_dataloader_params[\"negative_filepath\"] = negative_file\n",
    "    dataloader = dis_dataloader_params(**dis_dataloader_params) #this is where data iterator is used\n",
    "\n",
    "    cross_entropy = nn.CrossEntropyLoss() #this one is similar to NLL (negative log likelihood)\n",
    "    if use_cuda:\n",
    "        cross_entropy = cross_entropy.cuda()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for _, sample in enumerate(dataloader):\n",
    "            d_optimizer.zero_grad()\n",
    "            data, label = sample[\"data\"], sample[\"label\"] #initialize sample variables\n",
    "            data = Variable(data)\n",
    "            label = Variable(label)\n",
    "            if use_cuda:\n",
    "                data = data.cuda()\n",
    "                label = label.cuda()\n",
    "            outs = discriminator(data)\n",
    "            loss = cross_entropy(outs[\"score\"], label.view(-1)) + discriminator.l2_loss()\n",
    "            d_lr_scheduler.step()\n",
    "            loss.backward()\n",
    "            d_optimizer.step()\n",
    "            print(\"Pre-Discriminator loss: {:.5f}\".format(loss))\n",
    "    \n",
    "    model_dict[\"discriminator\"] = discriminator\n",
    "    optimizer_dict[\"discriminator\"] = d_optimizer\n",
    "    scheduler_dict[\"discriminator\"] = d_lr_scheduler\n",
    "    return model_dict, optimizer_dict, scheduler_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adversarial training \n",
    "def adversarial_train(model_dict, optimizer_dict, scheduler_dict, dis_dataloader_params,\n",
    "                      vocab_size, pos_file, neg_file, batch_size, gen_train_num=1,\n",
    "                      dis_train_epoch=5, dis_train_num=3, max_norm=5.0,\n",
    "                      rollout_num=4, use_cuda=False, temperature=1.0):\n",
    "    \"\"\"\n",
    "        Get all the models, optimizer and schedulers\n",
    "    \"\"\"                     \n",
    "    generator = model_dict[\"generator\"]\n",
    "    discriminator = model_dict [\"discriminator\"]\n",
    "    worker = generator.worker\n",
    "    manager = generator.manager\n",
    "\n",
    "    m_optimizer = optimizer_dict[\"manager\"]\n",
    "    w_optimizer = optimizer_dict[\"worker\"]\n",
    "    d_optimizer = optimizer_dict[\"discriminator\"]\n",
    "\n",
    "    #Why zero grad only m and w?\n",
    "    m_optimizer.zero_grad()\n",
    "    w_optimizer.zero_grad()\n",
    "\n",
    "    m_lr_scheduler = scheduler_dict[\"manager\"]\n",
    "    w_lr_scheduler = scheduler_dict[\"worker\"]\n",
    "    d_lr_scheduler = scheduler_dict[\"discriminator\"]\n",
    "\n",
    "    #Adversarial training for generator\n",
    "\n",
    "    for _ in range(gen_train_num):\n",
    "        m_lr_scheduler.step()\n",
    "        w_lr_scheduler.step()\n",
    "\n",
    "        m_optimizer.zero_grad()\n",
    "        w_optimizer.zero_grad()\n",
    "\n",
    "        #get all the return values\n",
    "        adv_rets = recurrent_func(\"adv\")(model_dict, use_cuda)\n",
    "        real_goal = adv_rets[\"real_goal\"]\n",
    "        all_goal = adv_rets[\"all_goal\"]\n",
    "        prediction = adv_rets[\"prediction\"]\n",
    "        delta_feature = adv_rets[\"delta_feature\"]\n",
    "        delta_feature_for_worker = adv_rets[\"delta_feature_for_worker\"]\n",
    "        gen_token = adv_rets[\"gen_token\"]\n",
    "\n",
    "        rewards = get_rewards(model_dict, gen_token, rollout_num, use_cuda)\n",
    "        m_loss = loss_func(\"adv_manager\")(rewards, real_goal, delta_feature)\n",
    "        w_loss = loss_func(\"adv_worker\")(all_goal, delta_feature_for_worker, gen_token, prediction, vocab_size, use_cuda)\n",
    "\n",
    "        torch.autograd.grad(m_loss, manager.parameters()) #based on loss improve the parameters\n",
    "        torch.autograd.grad(w_loss, worker.parameters())\n",
    "        clip_grad_norm(manager.parameters(), max_norm)\n",
    "        clip_grad_norm(worker.parameters(), max_norm)\n",
    "        m_optimizer.step()\n",
    "        w_optimizer.step()\n",
    "        print(\"Adv-Manager loss: {:.5f} Adv-Worker loss: {:.5f}\".format(m_loss, w_loss))\n",
    "    \n",
    "    del adv_rets\n",
    "    del real_goal\n",
    "    del all_goal\n",
    "    del prediction\n",
    "    del delta_feature\n",
    "    del delta_feature_for_worker\n",
    "    del gen_token\n",
    "    del rewards\n",
    "\n",
    "    #Adversarial training for discriminator\n",
    "    for _ in range(dis_train_epoch):\n",
    "        generate_samples(model_dict, neg_file, batch_size, use_cuda, temperature)\n",
    "        dis_dataloader_params[\"positive_filepath\"] = pos_file\n",
    "        dis_dataloader_params[\"negative_filepath\"] = neg_file\n",
    "        dataloader = dis_data_loader(**dis_dataloader_params)\n",
    "\n",
    "        cross_entropy = nn.CrossEntropyLoss()\n",
    "        if use_cuda:\n",
    "            cross_entropy = cross_entropy.cuda()\n",
    "        \"\"\"\n",
    "        for d-steps do\n",
    "            Use current G, θm,θw to generate negative examples and combine with given positive examples S \n",
    "            Train discriminator Dφ for k epochs by Eq. (2)\n",
    "        end for\n",
    "        \"\"\"\n",
    "        for _ in range(dis_train_num): \n",
    "            for i, sample in enumerate(dataloader):\n",
    "                data, label = sample[\"data\"], sample[\"label\"]\n",
    "                data = Variable(data)\n",
    "                label = Variable(label)\n",
    "                if use_cuda:\n",
    "                    data = data.cuda(async=True)\n",
    "                    label = label.cuda(async=True)\n",
    "                outs = discriminator(data)\n",
    "                loss = cross_entropy(outs[\"score\"], label.view(-1)) + discriminator.l2_loss()\n",
    "                d_optimizer.zero_grad()\n",
    "                d_lr_scheduler.step()\n",
    "                loss.backward()\n",
    "                d_optimizer.step()\n",
    "                print(\"Adv-Discriminator Loss: {:.5f}\".format(loss))\n",
    "    #Save all changes\n",
    "    model_dict[\"discriminator\"] = discriminator\n",
    "    generator.worker = worker\n",
    "    generator.manager = manager\n",
    "    model_dict[\"generator\"] = generator\n",
    "\n",
    "    optimizer_dict[\"manager\"] = m_optimizer\n",
    "    optimizer_dict[\"worker\"] = w_optimizer\n",
    "    optimizer_dict[\"discriminator\"] = d_optimizer\n",
    "\n",
    "    scheduler_dict[\"manager\"] = m_lr_scheduler\n",
    "    scheduler_dict[\"worker\"] = w_lr_scheduler\n",
    "    scheduler_dict[\"disciminator\"] = d_lr_scheduler\n",
    "\n",
    "    return model_dict, optimizer_dict, scheduler_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model_dict, optimizer_dict, scheduler_dict, ckpt_num, replace=False):\n",
    "    file_name = \"checkpoint\" + str(ckpt_num) + \".pth.tar\"\n",
    "    torch.save({\"model_dict\": model_dict, \"optimizer_dict\": optimizer_dict, \"scheduler_dict\": scheduler_dict, \"ckpt_num\": ckpt_num}, file_name)\n",
    "    if replace:\n",
    "        ckpts = glob.glob(\"checkpoint*\")\n",
    "        ckpt_nums = [int(x.split('.')[0][10:]) for x in ckpts]\n",
    "        oldest_ckpt = \"checkpoint\" + str(min(ckpt_nums)) + \".pth.tar\"\n",
    "        os.remove(oldest_ckpt)\n",
    "\n",
    "def restore_checkpoint(ckpt_path):\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Get all parameters\n",
    "    \"\"\"\n",
    "    param_dict = get_arguments()\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    #Random seed\n",
    "    torch.manual_seed(param_dict[\"train_params\"][\"seed\"])\n",
    "    #Pretrain step\n",
    "    checkpoint_path = param_dict[\"train_params\"][\"checkpoint_path\"]\n",
    "    if checkpoint_path is not None:\n",
    "        checkpoint = restore_checkpoint(checkpoint_path)\n",
    "        model_dict = checkpoint[\"model_dict\"]\n",
    "        optimizer_dict = checkpoint[\"optimizer_dict\"]\n",
    "        scheduler_dict = checkpoint[\"scheduler_dict\"]\n",
    "        ckpt_num = checkpoint[\"ckpt_num\"]\n",
    "    else:\n",
    "        model_dict = prepare_model_dict(use_cuda)\n",
    "        lr_dict = param_dict[\"train_params\"][\"lr_dict\"]\n",
    "        optimizer_dict = prepare_optimizer_dict(model_dict, lr_dict)\n",
    "        gamma = param_dict[\"train_params\"][\"decay_rate\"]\n",
    "        step_size = param_dict[\"train_params\"][\"decay_step_size\"]\n",
    "        scheduler_dict = prepare_scheduler_dict(optimizer_dict, gamma=gamma, step_size=step_size)\n",
    "    #Pretrain discriminator\n",
    "    print (\"#########################################################################\")\n",
    "    print (\"Start Pretraining Discriminator...\")\n",
    "    dis_data_params = param_dict[\"dis_data_params\"]\n",
    "    if use_cuda:\n",
    "        dis_data_params[\"pin_memory\"] = True\n",
    "    pos_file = dis_data_params[\"positive_filepath\"]\n",
    "    neg_file = dis_data_params[\"negative_filepath\"]\n",
    "    batch_size = param_dict[\"train_params\"][\"generated_num\"]\n",
    "    vocab_size = param_dict[\"leak_gan_params\"][\"discriminator_params\"][\"vocab_size\"]\n",
    "    for _ in range(param_dict[\"train_params\"][\"pre_dis_epoch_num\"]):\n",
    "        model_dict, optimizer_dict, scheduler_dict = pretrain_discriminator(model_dict, optimizer_dict, scheduler_dict, dis_data_params, vocab_size=vocab_size, positive_file=pos_file, negative_file=neg_file, batch_size=batch_size, epochs=1, use_cuda=use_cuda)\n",
    "    #Pretrain generator \n",
    "    print (\"#########################################################################\")\n",
    "    print (\"Start Pretraining Generator...\")\n",
    "    real_data_params = param_dict[\"real_data_params\"]\n",
    "    if use_cuda:\n",
    "        real_data_params[\"pin_memory\"] = True\n",
    "    r_dataloader = real_data_loader(**real_data_params)\n",
    "    for _ in range(param_dict[\"train_params\"][\"pre_gen_epoch_num\"]):\n",
    "        model_dict, optimizer_dict, scheduler_dict = pretrain_generator(model_dict, optimizer_dict, scheduler_dict, r_dataloader, vocab_size=vocab_size, use_cuda=use_cuda)\n",
    "    #Finish pretrain and save the checkpoint\n",
    "    ckpt_num = 0\n",
    "    save_checkpoint(model_dict, optimizer_dict, scheduler_dict, ckpt_num)\n",
    "\n",
    "    #Adversarial train of D and G\n",
    "    print (\"#########################################################################\")\n",
    "    print (\"Start Adversarial Training...\")\n",
    "    save_num = param_dict[\"train_params\"][\"save_num\"] #save checkpoint after this number of repetitions\n",
    "    replace_num = param_dict[\"train_params\"][\"replace_num\"]\n",
    "\n",
    "    for epoch in range(param_dict[\"train_params\"][\"total_epoch\"]):\n",
    "        model_dict, optimizer_dict, scheduler_dict = adversarial_train(model_dict, optimizer_dict, scheduler_dict, dis_data_params, vocab_size=vocab_size, pos_file=pos_file, neg_file=neg_file, batch_size=batch_size, use_cuda=use_cuda)\n",
    "        if (epoch + 1) % save_num == 0:\n",
    "            ckpt_num += 1\n",
    "            if ckpt_num % replace_num == 0:\n",
    "                save_checkpoint(model_dict, optimizer_dict, scheduler_dict, ckpt_num, replace=True)\n",
    "            else:\n",
    "               save_checkpoint(model_dict, optimizer_dict, scheduler_dict, ckpt_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
